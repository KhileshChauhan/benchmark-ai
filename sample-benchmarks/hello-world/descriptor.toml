# BenchmarkAI meta
spec_version = "0.1.0"

# These fields don't have any impact on the job to run, they contain
# merely informative data so the benchmark can be categorized when displayed
# in the dashboard.
[info]
description = """ \
    A hello world example of using Benchmark AI\
    """

# Labels for metrics
[info.labels]
# Labels and values must be 63 characters or less, beginning and ending with an alphanumeric character
# ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between
# task_name is a mandatory label which will be exported as a dimension for this job's metrics
task_name = "hello_world_example"

# 1. Hardware
[hardware]
instance_type = "t3.small"
strategy = "single_node"
# In us-east-1, use1-az5 az that has plenty of t3s. Adjust for your installation's region.
#aws_zone_id = "use1-az5"

# 2. Environment
[env]
# Docker hub <hub-user>/<repo-name>:<tag> 
docker_image = "benchmarkai/hello-world:latest"

# 3. Machine learning related settings: 
# dataset, benchmark code and parameters it takes
[ml]
benchmark_code = "python3 hello-world.py"

# 4. Output
[output]
# [Opt] Custom metrics descriptions
# List all required metrics descriptions below.
# Make an entry in same format as the one below.
[[output.metrics]]
# Name of the metric that will appear in the dashboards.
name = "elapsed"
# Metric unit (required)
units = "ms"
# Pattern for log parsing for this metric.
# This is a literal string: use SINGLE QUOTES
pattern = 'elapsed: ([-+]?\d*\.\d+|\d+)'