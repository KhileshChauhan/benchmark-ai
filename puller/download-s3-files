#!/bin/bash

# ensure this script exits with an error code
# if any of the commands fail - this script is run
# inside an initContainer - so the pod should fail
# in case of errors
set -e

BUCKET=$1

declare -a HELP=("--help" "-h" "help")
if [[ " ${HELP[*]} " == *" $BUCKET "* ]]; then
    echo "Usage: $0 <S3Bucket> <FILE1>:<FILE2>:..."
    echo "  <Files> format: <S3Path>[,Permissions][,DOWNLOADPATH][,unpack]"
    exit 0
fi

USER_FILES=$2
FILES=(${USER_FILES//:/ })

S3_ENDPOINT_ARG=""
[ "${S3_ENDPOINT}" != "" ] && S3_ENDPOINT_ARG="--endpoint-url=${S3_ENDPOINT}"

#Ugly HACK to check if should be a dir.
#Brittle and unstable - yes, fails for filenames with PRE
is_prefix(){
    local S3FILE=$1
    pre=$(aws s3 ls s3://${BUCKET}/${S3FILE} ${S3_ENDPOINT_ARG} | grep " PRE ")
    [ -n "${pre}" ] || return 1
}

# There is a race condition in kube2iam, which can lead to pods
# executing before they get assigned the AWS role.
# https://github.com/jtblin/kube2iam/issues/136
# wait until s3 can be reached, or timeout.
function wait_for_s3 {
    # waits up to 5 minutes...
    local max_attempts=30
    local timeout=10
    local attempt=0
    local exit_code=0

    while [[ $attempt < $max_attempts ]]; do
        echo "Attempting to reach S3..."
        aws s3 ls s3://${BUCKET} > /dev/null 2>&1
        exit_code=$?

        if [[ $exit_code == 0 ]]; then
            echo "Reached S3...continuing..."
            return 0
        fi

        echo "Failure! Retrying in ${timeout} seconds..." 1>&2
        sleep $timeout
        attempt=$(( attempt + 1 ))
    done

    echo "Could not reach S3...exiting..."
    exit $exit_code
}

# Make sure s3 can be reached before continuing...
wait_for_s3

for file in "${FILES[@]}"; do
    PIECES=(${file//,/ })

    S3FILE=${PIECES[0]}
    PERMS=${PIECES[1]}
    TARGET=${PIECES[2]}
    UNPACK=${PIECES[3]}

    echo "${file}"

    if [ "${S3FILE}" == "" ]; then
        echo " ---> No S3 file given for target ${file}, skipping"
        continue
    fi

    if [ "${TARGET}" == "" ]; then
        TARGET="/data/${S3FILE}"
        echo " ---> No download path for ${file}, assuming ${TARGET}"
    fi

    CMD="aws s3 cp s3://${BUCKET}/${S3FILE} ${TARGET} ${S3_ENDPOINT_ARG}"
    is_prefix "$S3FILE" && CMD="aws s3 sync s3://${BUCKET}/${S3FILE} ${TARGET} ${S3_ENDPOINT_ARG}"
    echo " ---> Executing: ${CMD}"
    eval $CMD

    if [ "${PERMS}" != "" ]; then
        echo " ---> Updating permissions ${TARGET}[${PERMS}]" 
        chmod ${PERMS} ${TARGET}
    fi

    if [ "${UNPACK}" != "" ]; then
        UNPACK_FOLDER=$(mktemp -d)
        tar -pxvf ${TARGET} -C ${UNPACK_FOLDER}

        if [ "${PERMS}" != "" ]; then
            echo " ---> Updating permissions ${UNPACK_FOLDER}[${PERMS}]"
            chmod ${PERMS} ${UNPACK_FOLDER}
            chmod -R ${PERMS} ${UNPACK_FOLDER}
        fi

        rm ${TARGET}

        if [ "${UNPACK}" == "unpack" ]; then
            mv ${UNPACK_FOLDER} ${TARGET}
        elif [ "${UNPACK}" == "unpack_in_place" ]; then
            mv -f ${UNPACK_FOLDER}/* $(dirname ${TARGET})
        fi

    fi
done
