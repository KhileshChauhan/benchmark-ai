apiVersion: v1
kind: Service
metadata:
  name: {inference_server_job_id}
  labels:
    action-id: {event.action_id}
    client-id: {event.client_id}
    created-by: {service_name}
spec:
  selector:
    inference_server_name: {inference_server_job_id}
  ports: []
---

apiVersion: batch/v1
kind: Job
metadata:
  name: {inference_server_job_id}
  labels:
    app: benchmark-ai
    action-id: {event.action_id}
    client-id: {event.client_id}
    created-by: {service_name}
    inference_server_name: {inference_server_job_id}
spec:
  template:
    metadata:
      labels:
        app: benchmark-ai
        action-id: {event.action_id}
        client-id: {event.client_id}
        created-by: {service_name}
        inference_server_name: {inference_server_job_id}
      annotations:
        iam.amazonaws.com/role: benchmark-host
    spec:
      serviceAccountName: metrics-pusher
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - benchmark-ai
            topologyKey: kubernetes.io/hostname
      initContainers:
      - name: data-puller
        image: {config.puller_docker_image}
        env:
          - name: S3_ENDPOINT
            valueFrom:
              configMapKeyRef:
                name: outputs-infrastructure
                key: s3_endpoint
          # This environment variables are optional.
          # They reference a config map missing in DEVO/PROD.
          # Enable the integration tests
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              configMapKeyRef:
                name: s3
                key: access-key-id
                optional: true
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              configMapKeyRef:
                name: s3
                key: secret-access-key
                optional: true
        volumeMounts:
          - name: datasets-volume
            mountPath: /data
      containers:
      - name: inference-server
        image: {descriptor.server.env.docker_image}
        resources:
          limits:
            nvidia.com/gpu: {descriptor.server.hardware.gpus_per_instance}
        command: []
        args: []
        securityContext:
          privileged: {descriptor.server.env.privileged}
      # If no client job can be found - delete yourself
      - name: client-lock
        image: {config.job_status_trigger_docker_image}
        env:
        - name: JOB_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: JOB_NAME
          value: {job_id}
        - name: TRIGGER_STATUSES
          value: "[SUCCEEDED, FAILED]"
        - name: COMMAND
          value: '/opt/env/bin/kubectl delete job,service {inference_server_job_id}'
        - name: JOB_NOT_FOUND_GRACE_PERIOD_SECONDS
          value: '30'
      nodeSelector:
        beta.kubernetes.io/instance-type: {descriptor.server.hardware.instance_type}
        node.type: bai-worker
        failure-domain.beta.kubernetes.io/zone: {availability_zone}
      restartPolicy: Never
      volumes:
      - name: datasets-volume
        emptyDir: {{}}
  backoffLimit: 4
